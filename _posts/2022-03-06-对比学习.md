---
title:      对比学习              # 标题 
date:       2022-02-06              # 时间
author:     vhwz                      # 作者
categories: [Algorithm, DeepLearning]
tags: [DeepLearning] 
math: true
---


## 对比学习

神经网络可以提取图像特征，一张图片的特征可以看作embedding space 中的一个点。

对比学习和度量学习的目的是使神经网络提取的特征对于相似图片距离尽可能接近，不相似图片尽可能远离。区别在于对比学习是无监督的，度量学习是有监督的。

<img src="/assets/img/old_blog/image-20220514142417513.png" alt="image-20220514142417513" style="zoom:50%;" />

对比学习需要定义特征之间的距离。特征之间的距离可以定义为L2距离、余弦相似度等。在训练时，设置Loss使相似图像的特征距离尽可能小，不相似图片特征距离尽可能大。

看起来在训练时，我们需要知道输入图片对是否相似，将其作为标签，其实这是度量学习的做法。

对比学习是一种无监督的学习方式，它通过设置代理任务(pretext task)避免了标签的使用。

常用的代理任务是个体判别(instance discrimination)，即对训练集中的某一张图片进行图像增强，将增强得到的图片视为同一类， 认为它们是相似的。数据集中所有其他图片都是不相似的。这样就自动生成了相似性标签。

### NCEloss and InfoNCE loss

考虑分类问题中包含softmax的cross entropy loss 公式：其中z是网络输出的score，k是类别数
$$
-\log \frac{\exp (z+)}{\sum_{i=0}^{k} \exp \left(z_{i}\right)}
$$
在一些任务中类别数非常大，因此分母中计算所有类别是不可行的(太费时间)，例如在word2vec中，单词的数量很大；在对比学习中，采用个体判别代理任务，每张图片都属于一个单独的类，类别数等于图片的数量。

Noise contrastive estimation (NCE)

将多分类转化为二分类问题：在经验分布上采样一个正类，在一个噪声分布上采样负类，对正类和负类进行二分类。



InfoNCE， moco中的infoNCE：
$$
L_{q}=-\log \frac{\exp \left(q \cdot k_{+} / \tau\right)}{\left.\sum_{i=0}^{k} \exp \left(q \cdot k_{i} / \tau\right)\right)}
$$

### InstDisc

提出 instance discrimination

使用NCELoss

### CPC

[4] Representation Learning with Contrastive Predictive Coding

### CMC

由 [5] Contrastive Multiview Coding 提出

将一个对象的不同视角照片（RGB图，深度图，分割图等，需要数据集的支持）(或者将图像转为lab格式，使用l通道和ab通道)作为正样本，其他作为负样本。对不同的视角使用不同的编码器

人类通过不同的传感器（视角）观察世界，例如：在红色灯光下看，用左眼看，用耳朵听等等。

每种视角都是有噪声的，不完整的。但是不同视角常常包含一些相同的信息：例如物理信息，几何形状，语义信息等。我们对一个经典的假设进行了调查：一个强有力的表示(representaion)应该对这些具有视角不变性的要素进行建模。

我们在多视角对比学习的框架下研究了这一假设。我们让不同视角的互信息最大化。

### SimCLR

[6] A simple Framework for Contrastive Learning of Visual Representations 提出



![image-20220516091631341](/assets/img/old_blog/image-20220516091631341.png)



### MoCo

MoCo由 Momentum Contrast for Unsupervised Visual representation learning 提出的对比学习算法。

它使用的代理任务是instance discrimination。它首先对输入图片进行两次随机图像增强，得到两张图像Xq和Xk。然后用encoder提取图像特征，记为q, k。使用余弦相似度作为特征距离，即$d = q^Tk$。

显然(Xq, Xk)是一个正样本对。为了进行对比学习，我们还需要一些负样本对。

按照上一节中对代理任务的描述，我们应该提取数据集中所有其他图片的特征作为负样本。但是这样做的计算量实在太大：每一个iter要对整个数据集进行一次前向传播。为了减小计算量，我们可以把整个数据集的特征都保存下来，在需要的时候直接拿来使用。我们将保存特征的地方称为memory bank。

memory bank减小了计算量，也带来了一个问题。假设我们在iter=1时保存了数据集的特征向量，在iter=10时，Xq用encoder提取特征得到q，取出memory bank中的特征用于构成负样本。然而encoder是会随着训练不断改变的，这时提取q使用的编码器和此前提取memory bank中特征使用的编码器已经不同。因此memory bank会带来q, k 特征不一致的问题。memory bank是在[1] Instdisc 中提出的。为了解决这个问题，我们需要用当前batch计算出的特征不断更新memory bank。此外，[1]中也额外设计了Loss函数来解决这一问题。

memory bank减小了前向传播的计算量，但当数据集比较大时，每次考虑整个数据集的特征，在计算Loss时的计算量也是很大的，因此我们可以从数据集中采样一些图片计算Loss，而不是考虑整个数据集。

为了解决计算量过大和特征一致性的问题，MoCo采用了另一种方式来构造负样本。如下图所示。

<img src="/assets/img/old_blog/image-20220514151739349.png" alt="image-20220514151739349" style="zoom:67%;" />

类似于memroy bank, 它设置了一个长度为K=65536的队列用于保存负样本特征，每一个iter都将新算出的负样本特征入队。

它专门设置了另一个编码器(momentum encoder)用于提取负样本的特征。momentum encoder 并不参与反向传播，但是仍然需要更新参数。MoCo对encoder 和 momentum encoder采用相同的网络结构，并利用encoder的参数更新momentum encoder。具体来说，记两个网络为 f_q, f_k, 在每个iter中，令

 $f_k.params = m*f_k.params+(1-m)*f_q.params$

其中m是超参数(momentum )。m被设为0.999, 这样f_k的更新将非常缓慢，这保证了队列中的特征是由相似的网络提取出的，增强了特征的一致性。

以下是一个可读性较强的伪代码，来自原始论文。

```python
# f_q, f_k: encoder networks for query and key
# queue: dictionary as a queue of K keys (CxK)
# m: momentum
# t: temperature
f_k.params = f_q.params # initialize
for x in loader: # load a minibatch x with N samples
    x_q = aug(x) # a randomly augmented version
    x_k = aug(x) # another randomly augmented version
    q = f_q.forward(x_q) # queries: NxC
    k = f_k.forward(x_k) # keys: NxC
    k = k.detach() # no gradient to keys
    # positive logits: Nx1
    l_pos = bmm(q.view(N,1,C), k.view(N,C,1))
    # negative logits: NxK
    l_neg = mm(q.view(N,C), queue.view(C,K))
    # logits: Nx(1+K)
    logits = cat([l_pos, l_neg], dim=1)
    # contrastive loss, Eqn.(1)
    #labels是类别编号 正类类别编号是0
    labels = zeros(N) # positives are the 0-th
    loss = CrossEntropyLoss(logits/t, labels)
    # SGD update: query network
    loss.backward()
    update(f_q.params)
    # momentum update: key network
    f_k.params = m*f_k.params+(1-m)*f_q.params
    # update dictionary
    enqueue(queue, k) # enqueue the current minibatch
    dequeue(queue) # dequeue the earliest minibatch

```

### MoCoV2

[7] Improved Baselines with Momentum Contrastive Learning

学习SimCLR，增加了MLP层

增强了数据增强

使用 cos learning rate schedule

### SimCLRv2

[8] Big Self-Supervised Models are Strong Semi-Supervised Learners

<img src="/assets/img/old_blog/image-20220516093417947.png" alt="image-20220516093417947" style="zoom:67%;" />

使用更大的模型和SKNet

MLP的层数 +1

使用了MoCo的动量编码器 

### SwAV

[9] Unsupervised Learning  of Visual Features by Contrasting Cluster Assignments

代理任务：不同视角特征接近 换位预测

对提取的特征聚类，利用聚类中心作为负样本

使用mulit-crop数据增强

![image-20220516095407370](/assets/img/old_blog/image-20220516095407370.png)



### BYOL

[10] Bootstrap Your Own Latent A New Approach to  self-supervised Learning

不需要负样本

代理任务

![image-20220516105906314](/assets/img/old_blog/image-20220516105906314.png)

### SimSiam

[11] Exploring Simple Siamese Representation Learning

网络结构如图所示。

<img src="/assets/img/old_blog/image-20220526155249174.png" alt="image-20220526155249174" style="zoom:67%;" />

没有使用负样本和动量编码器

```python
# f: backbone + projection mlp
# h: prediction mlp
for x in loader: # load a minibatch x with n samples
    x1, x2 = aug(x), aug(x) # random augmentation
    z1, z2 = f(x1), f(x2) # projections, n-by-d
    p1, p2 = h(z1), h(z2) # predictions, n-by-d
    L = D(p1, z2)/2 + D(p2, z1)/2 # loss
    L.backward() # back-propagate
    update(f, h) # SGD update
def D(p, z): # negative cosine similarity
    z = z.detach() # stop gradient
    p = normalize(p, dim=1) # l2-normalize
    z = normalize(z, dim=1) # l2-normalize
    return -(p*z).sum(dim=1).mean()
```



### Barlow Twins



### MoCov3

[12] An Empirical Study of Training Self-Supervised Vision Transformer

用vit替换了cnn

发现在vit训练中，冻结token层

```python
# f_q: encoder: backbone + proj mlp + pred mlp
# f_k: momentum encoder: backbone + proj mlp
# m: momentum coefficient
# tau: temperature
for x in loader: # load a minibatch x with N samples
    x1, x2 = aug(x), aug(x) # augmentation
    q1, q2 = f_q(x1), f_q(x2) # queries: [N, C] each
    k1, k2 = f_k(x1), f_k(x2) # keys: [N, C] each
    loss = ctr(q1, k2) + ctr(q2, k1) # symmetrized
    loss.backward()
    update(f_q) # optimizer update: f_q
    f_k = m*f_k + (1-m)*f_q # momentum update: f_k
# contrastive loss
def ctr(q, k):
    logits = mm(q, k.t()) # [N, N] pairs
    labels = range(N) # positives are in diagonal
    loss = CrossEntropyLoss(logits/tau, labels)
    return 2 * tau * loss
#Notes: mm is matrix multiplication. k.t() is k’s transpose. The prediction head is excluded from f k (and thus the momentum update).
```



### DINO: self-distillation with on labels

[13] Emerging Properites in self-Supervised Vision Transformer

ema: exponential moving average

<img src="/assets/img/old_blog/image-20220526162045709.png" alt="image-20220526162045709" style="zoom:67%;" />



```python
# gs, gt: student and teacher networks
# C: center (K)
# tps, tpt: student and teacher temperatures
# l, m: network and center momentum rates
gt.params = gs.params
for x in loader: # load a minibatch x with n samples
    x1, x2 = augment(x), augment(x) # random views
    s1, s2 = gs(x1), gs(x2) # student output n-by-K
    t1, t2 = gt(x1), gt(x2) # teacher output n-by-K
    loss = H(t1, s2)/2 + H(t2, s1)/2
    loss.backward() # back-propagate
    # student, teacher and center updates
    update(gs) # SGD
    gt.params = l*gt.params + (1-l)*gs.params
    C = m*C + (1-m)*cat([t1, t2]).mean(dim=0)
def H(t, s):
    t = t.detach() # stop gradient
    s = softmax(s / tps, dim=1)
    t = softmax((t - C) / tpt, dim=1) # center + sharpen
    return - (t * log(s)).sum(dim=1).mean()

```
CAE: Context AutoEncoder for Self-Supervised Representation Learning
https://github.com/lxtGH/CAE
https://github.com/open-mmlab/mmselfsup
### ref

1. Unsupervised feature learning via non-parametric instance discrimination.
2. Momentum Contrast for Unsupervised Visual representation learning
3. https://www.bilibili.com/video/BV1C3411s7t9
4. [Representation Learning with Contrastive Predictive Coding](https://arxiv.org/abs/1807.03748)
5. [Contrastive Multiview Coding](https://arxiv.org/abs/1906.05849)
6. A simple Framework for Contrastive Learning of Visual Representations
7. Improved Baselines with Momentum Contrastive Learning
8. Big Self-Supervised Models are Strong Semi-Supervised Learners
9. Unsupervised Learning  of Visual Features by Contrasting Cluster Assignments
10. Bootstrap Your Own Latent A New Approach to  self-supervised Learning
11. Exploring Simple Siamese Representation Learning
12. An Empirical Study of Training Self-Supervised Vision Transformer
13. Emerging Properites in self-Supervised Vision Transformer


